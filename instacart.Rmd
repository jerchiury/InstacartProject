---
title: "Instacart Analysis"
author: "Jerry Chiu"
date: "February 7, 2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
setwd('C:/Users/Jerry/Desktop/Jerry/projects/instacart')
library(ggplot2)
library(dplyr)
library(reshape2)
library(textclean)
library(textstem)
library(stringr)
library(stopwords)
library(tm)
library(openNLP)
library(wordcloud)
library(arules)
library(cluster)
library(treemap)
library(RColorBrewer)
library(ngram)
library(data.table)
library(plotly)
library(ggrepel)
library(textmineR)
library(factoextra)
library(NbClust)
library(dbscan)
library(fpc)
library(tidyr)
library(tidyverse)
library(dtw)
set.seed(1)
options(dplyr.summarise.inform = FALSE)
options(warn=-1)
```

## Abstract

The main objective of this project is the mining of relevant association rules for use in hypothetical online recommenders. This is not just a simple mining of all association rules; this project will take into account the flexibility, the usefullness, and the importance of the association rules mined to provide new perspectives in designing recommenders. 

The analysis takes the following steps:

1. Data understanding and data integrity check 
2. Data cleaning, data exploration, and statistical analysis
3. Feature Engineering
4. Clustering of products
5. Mining of association rules
6. Exploration of the association rules
7. Summary

## 1. Data Understanding and integrity check

The data comes from [Instacart](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2), an online shopping platform. The datasets used in this project are:

* aisles.csv
* departments.csv
* products.csv
* orders.csv
* order_products__prior.csv

```{r}
aisles=read.csv('aisles.csv')
departments=read.csv('departments.csv')
products=read.csv('products.csv')
orders=read.csv('orders.csv')
order_pp=read.csv('order_products__prior.csv')

# see if any cell is just white space
white.space=function(data){
  return(sum(grepl('^[ ]*$',data)))}

# aisles.csv
colSums(is.na(aisles))
sapply(aisles, white.space)

# departments
colSums(is.na(departments))
sapply(departments, white.space)

# orders
colSums(is.na(orders)) #206209, all from days_since_prior_order
sapply(orders, white.space)

# products
colSums(is.na(products))
sapply(products, white.space)

# order_pp
colSums(is.na(products))
```

we see that there are no white space or NA's except for the 206209 NA's in orders.csv, let's see why.

```{r}
head(orders[which(is.na(orders$days_since_prior_order)),])
```

we see that the order_id is quite random, but the user_id goes up and is in order, and order_number is always 1. let's guess that the relationship is that days_since_prior_order is NA exactly when order_number is 1

```{r}
nrow(filter(orders, is.na(days_since_prior_order), order_number!=1))
```

we get nothing such that order number is not 1 and days_since_prior_order is NA, confirming the conjecture

Now the data integrity is checked, we can go onto understand and explore it.

products=read.csv('products_clean.csv')
products.clust=read.csv('products_clust.csv')
clusters=read.csv('cluster.csv')
aisle.dep=read.csv('aisle_dep.csv')
order_size=read.csv('order_size.csv')


# aisles.csv

```{r}
str(aisles)
head(aisles)
```

aisles.csv contains the aisle ID and aisle name that the products are sorted into. Total of 134 aisles. Aisle 6 looks peculiar as it is labeled "other", and aisles 100 is named "missing". Otherwise, nothing interesting yet. 

# departments.csv

```{r}
str(departments)
head(departments)
```

departments.csv contains the department ID and department name that the products are sorted into. Total of 21 departments. Department 2 looks peculiar as it is labeled "other", and department 21 is named "missing". Otherwise, nothing interesting yet.

# orders

```{r}
str(orders) 
```

orders.csv contains the order inormation of each user. It contains the user id off each customer, the order id or each order, the day and hour the order is placed, the order number of each user (chronologically), and the gep between two off the user's consecutive orders (days since prior order). Let's look at some stats starting with the number of orders each user has placed.

```{r}
test=orders%>%group_by(user_id)%>%summarise(max=max(order_number))
summary(test$max)
hist(test$max, main='Histogram of number of orders per user', xlab='days')
```

Looks like a small outlier of supershoppers with 100 orders...
next, we examine the days of the week the orders are placed. 

```{r}
hist(orders$order_dow, main='Histogram of orders per day of week', xlab='day of week')
```

Looks like day of week 0 and 1 are the most popular. Does the avg user order on a particular dow?

```{r}
test1=orders%>%group_by(user_id, order_dow)%>%summarise(n=n())
test=test1%>%group_by(user_id)%>%summarize(max=1/max(n/sum(n)))
summary(test$max)
hist(test$max, main='Histogram of how many days a week users submit orders', xlab='number of days')
```

The result is such that the avg user shops about 3 main days of the week, reasonably spread. Now let's look at the hour of order, same processes. 

```{r}
summary(orders$order_hour_of_day) # 0 to 23
hist(orders$order_hour_of_day, main='Histogram of orders per hour of day', xlab='hour of day')
```

The peak shopping time is about 10 to 15 o'clock. Do ppl have a tendency to order at certain time of day?

```{r}
test=orders%>%group_by(user_id)%>%summarise(mean=mean(order_hour_of_day), sd=sd(order_hour_of_day))
summary(test$mean) #mean of 13.59. about 1 oclock
hist(test$mean, main='Histogram of summarized mean of order hour per user', xlab='hour of day')
```

The average standard deviation is 3.5 hours, meaning on average, the users usually orders within 3.5 hours of their previous orders, somewhat consistent. Now the heatmap. 

```{r}
orders.heat=orders%>%group_by(order_dow, order_hour_of_day)%>%summarise(n=n())

ggplot(data=orders.heat)+
  geom_tile(aes(x=order_dow,y=order_hour_of_day, fill=n))+
  scale_fill_gradient(high='red', low='green')+
  labs(title="heatmap of orders of a week", x='day of week', y='hour of day')
```

Let's see about day since prior order. 

```{r}
summary(orders$days_since_prior_order, na.rm=T) 
hist(orders$days_since_prior_order, main='Histogram of days since prior order', xlab='days')
median(filter(orders, days_since_prior_order!=30)$days_since_prior_order, na.rm=T)
```

Looks like there is a spike at 30 days, this is the cap of the data. The median is 7 days since prior order. 
Do ppl have a tendency to order in certain day intervals? I think a log graph of the frequency of the histogram would help.

```{r}
no30=filter(orders, days_since_prior_order!=30 & !is.na(days_since_prior_order)) #removing 30 days since it is a hard cap
test=hist(no30$days_since_prior_order, plot=F)
plot(x=1:29, y=log(test$counts), type='line', main='logged histogram of days since prior order', xlab='days', ylab='log of frequency')
```

Looks quite nice with the peaks at multiples of 7, and the decay after is quite predictable
In orders, we learned that users usually have 2 to 3 days of the week that they place orders on, and the time of day they do so is within 3.5 hours of each other. We also learned that there is clear signs of weekly ordering (orders of 7, 14, 21, 28 days apart). 

# products

```{r}
str(products)
```

products.csv contains the product id, name, the aisle the product belongs to, and the department. The first interest here is the relationship between aisle and department. Each aisle belongs to only 1 department. Let's see the relationship. 
aisle.dep=unique(products[c('aisle_id','department_id')])

```{r}
test=products%>%group_by(department_id, aisle_id)%>%summarise(n=n())
test$department_id=departments[test$department_id,'department']
test$aisle_id=aisles[test$aisle_id,'aisle']
treemap(test, index=c('department_id', 'aisle_id'), vSize='n', title='Departments and Aisles', overlap.labels = 1, fontsize.title = 24, fontsize.labels=c(20,15), ymod.labels=c(0.3,0))
```

Now, let's see about the product names, cleaning it, to prepare it for clustering. We do this by comparing the most common adjectives shared among the different aisles.

```{r}
unique.words=function(data){
  temp=data$product_name
  temp=Boost_tokenizer(temp)
  temp=unique(temp)
  
  return(data.frame(temp))}

mystop=stopwords::stopwords(language='en')
products=inner_join(products, aisles)
products=inner_join(products, departments)
product.names=products
product.names$product_name=gsub('[^A-z]', ' ', product.names$product_name)
product.names$product_name=removePunctuation(product.names$product_name)
product.names$product_name=lemmatize_strings(product.names$product_name)
product.names$product_name=gsub('[^A-z]', ' ', product.names$product_name)
product.names$product_name=gsub(" *\\b[[:alpha:]]{1}\\b *", " ", product.names$product_name)
product.names$product_name=trimws(product.names$product_name)
product.names$product_name=gsub(' +',' ',product.names$product_name)
product.names$product_name=tolower(product.names$product_name)
product.names$product_name=removeWords(product.names$product_name, mystop)
aisle.product.names=product.names%>%group_by(aisle)%>%group_modify(~unique.words(.x))
# now we have all the unique words of every aisle, we can tally
aisle.token.table=table(aisle.product.names$temp)
aisle.token.table=aisle.token.table/max(aisle.token.table)
aisle.token.table=aisle.token.table[order(aisle.token.table, decreasing=T)]
#we only want to filter out the non-nouns
# we use NLP tagging to find all the non-nouns in the descriptor 
top=data.frame(word=tolower(names(aisle.token.table)), freq=as.vector(aisle.token.table))

annotate.word=function(word){
  temp=annotate(word, list(Maxent_Sent_Token_Annotator(), Maxent_Word_Token_Annotator()))
  temp=annotate(word, Maxent_POS_Tag_Annotator(), temp)
  temp=subset(temp, type=='word')
  tags=sapply(temp$features, '[[', 'POS')
  return(tags)
}

tags=annotate.word(top$word)
top=cbind(top, tags)
top=top[order(top$freq, decreasing=T),]
# since the NLP tags are not perfect (orange got tagged as adj but we do have some oranges in produce), we manually filter

top[which(top$word%in%c('pack', 'style', 'flavor', 'mix','light','blend','size','ct','mini','oz')),'tags']='JJ'
top[which(top$word%in%c('orange','almond','french','olive')),'tags']='NN'

topJJ=subset(top, !tags%in%c('NN','DT','TO'))
topJJ=topJJ[1:50,]

aisle.token.table=subset(aisle.token.table, names(aisle.token.table)%in%topJJ$word)
par(bg="black")
wordcloud(words=topJJ$word, freq=topJJ$freq, scale=c(4,-2), rot.per=F, random.order=F, use.r.layout=T, colors=brewer.pal(8,"Accent"), random.color=F) #top words
```

we see that free, natural, organic, original are the our biggest common words between all the aisles. We can filter them out along with some others when we get to the analysis part.

count.ngrams=function(data, n, nword='all'){
  temp=data$product_name
  temp=removeWords(temp, top10)
  temp=gsub(" *\\b[[:alpha:]]{1}\\b *", " ", temp)
  temp=trimws(temp)
  temp=gsub(' +', ' ',temp)
  temp=temp[which(sapply(strsplit(temp, " "), length)>=n)]
  temp=ngram(temp, n=n)
  temp=get.phrasetable(temp)
  if(is.numeric(nword)){
    temp=temp[1:nword,]}
  return(temp)}

ngram.treemap=function(data, n, nword){
  ngram.dep=data%>%group_by(department_id)%>%group_modify(~count.ngrams(.x, n=n, nword=nword))
  ngram.dep$department_id=str_to_title(departments[ngram.dep$department_id,'department'])
  ngram.dep$size=1
  png(filename=paste0("dep_", n, "gram_treemap.png"),width=1920, height=1080)
  treemap(ngram.dep, index=c('department_id', 'ngrams'), algorithm='pivotSize', vSize='size', title=paste0('Top ', nword,' Frequent ', n, '-grams of each Department'), overlap.labels = 1, 
          fontsize.title = 20, fontsize.labels=c(25,18), bg.labels=0, fontcolor.labels=c('white','black'), force.print.labels=T, ymod.labels=c(0.3,0))
  dev.off()
  }

for(i in c(1,2,3,4)){ngram.treemap(product.names, i, 10)}
# variance of product per aisle based on product name only, the higher the variance, the lower the name diversity
prod.var=product.names%>%group_by(department_id, aisle_id)%>%group_modify(~count.ngrams(.x, n=1))
prod.var=prod.var%>%group_by(department_id, aisle_id)%>%summarise(var=1/(var(prop)+0.00000001))
prod.var$department_id=str_to_title(departments[prod.var$department_id,'department'])
prod.var$aisle_id=aisles[prod.var$aisle_id,'aisle']

png(filename="aisle_name_var.png",width=1920, height=1080)
treemap(prod.var, index=c('department_id', 'aisle_id'), algorithm='pivotSize', vSize='var', title="Variance of 1-gram of aisles", overlap.labels = 1, 
        fontsize.title = 20, fontsize.labels=c(25,18), bg.labels=0, fontcolor.labels=c('white','black'), force.print.labels=T, ymod.labels=c(0.3,0))
dev.off()

# huge variation in the missing and other dep and aisle. 

# putting missing dep and aisle into real dep and aisle
# if we know aisle, we know its department already, so really, gotta put them into correct aisle
# we first construct a table of all 1, 2 and 3-grams of each aisle
product.names$product_name=removeWords(product.names$product_name, topJJ$word)
product.names$product_name=gsub(" *\\b[[:alpha:]]{1}\\b *", " ", product.names$product_name)
product.names$product_name=trimws(product.names$product_name)
product.names$product_name=gsub(' +', ' ',product.names$product_name)

get.aisle.phrases=function(data){
  phrase=data.frame()
  for(n in 1:3){
    temp=data$product_name
    temp=temp[which(sapply(strsplit(temp, " "), length)>=n)]
    temp=ngram(temp, n=n)
    temp=get.phrasetable(temp)
    temp$ngrams=trimws(temp$ngrams)
    temp$phrase.length=n
    phrase=rbind(phrase, temp)}
  
  return(phrase)}

predict.aisle=function(data){
  pname=data$product_name
  nword=sapply(strsplit(pname, " "), length)
  if(nword){}
  match.found=F
  while(!match.found & nword>0){
    temp=ngram(pname, n=nword)
    temp=get.ngrams(temp)
    candidates=aisle.phrases[which(aisle.phrases$phrase.length==nword),]
    candidates=filter(candidates, ngrams%in%temp)
    match.found=ifelse(nrow(candidates)==0, F, T)
    if(match.found){
      pred=candidates%>%group_by(aisle_id)%>%summarise(score=sum(freq))
      pred=pred[order(pred$score, decreasing=T)[1],'aisle_id']
    }else{
      nword=nword-1}
  }
  if(!exists('pred')){return(100)}else{return(pred)}
}

aisle.phrases=filter(product.names, aisle!='missing')%>%group_by(aisle_id)%>%group_modify(~get.aisle.phrases(.x))

missing.aisle=filter(product.names, aisle=='missing')

test=product.names[sample(nrow(product.names),size=3000),]

for(i in 1:nrow(test)){
  test[i,'pred']=predict.aisle(test[i,])
  print(i)
}


length(which(test$pred==test$aisle_id))/3000 #91% accuracy


for(i in 1:nrow(missing.aisle)){
  missing.aisle[i,'aisle_id']=predict.aisle(missing.aisle[i,])
  print(i)
  }

aisle.dep=unique(products[c('aisle_id','aisle','department_id','department')])
aisle.dep=aisle.dep[order(aisle.dep$aisle_id),]
missing.aisle[c('aisle','department_id','department')]=
  aisle.dep[missing.aisle$aisle_id, c('aisle','department_id','department')]
test=product.names
product.names[which(product.names$aisle=='missing'),]=missing.aisle

write.csv(product.names, 'products_clean.csv', row.names=F)

#------------------------------------ orders_pp ----------------------------------------
str(order_pp) #all ints, a summary makes no sense since they are all mostly categorical integers
#---------------- order_id, the sensible thing here is to see the stats of order size
order_size=order_pp%>%group_by(order_id)%>%summarise(n=max(add_to_cart_order))
summary(order_size) # median 8, mean 10.09
sd(order_size) # 7.525, which is a lot

outlier.range=function(data, mult=1.5){
  low=quantile(data, 0.25, na.rm=T)
  high=quantile(data, 0.75, na.rm=T)
  iqr=IQR(data, na.rm=T)
  temp=c(low-mult*iqr, high+mult*iqr)
  return(as.vector(temp))}

order_size=order_size[which(order_size$n%between%outlier.range(order_size$n, 2)),]
order_size=inner_join(orders[c('order_id','user_id')],order_size)
order_size=order_size%>%group_by(n)%>%summarise(num.order=n(), num.user=length(unique(user_id)))


ggplot()+
  geom_bar(data=order_size, stat='identity', aes(x=n, y=num.order, fill=num.user))+
  labs(title='Number of orders per order size', x='Order Size', y='Number of Orders')+
  scale_fill_gradient(name = "Number of Users", low='#0027fc', high='#f71b1b')
  

#---------------- product_id
length(which(!order_pp$product_id %in% products$product_id)) # no unknown product id's
# treemap of aisle and dep based on number of orders, lets inner join the products with order_pp
order_pp=inner_join(order_pp, products, by='product_id')

order_size=order_pp%>%group_by(department, aisle)%>%summarise(n=n(), reordered=sum(reordered), reorder_pct=reordered/n*100)

png(filename="product_order_reorder.png",width=1920, height=1080)
treemap(order_size, index=c('department', 'aisle'), algorithm='pivotSize', type='value', vSize='n', vColor='reorder_pct', title="Number of orders and reorders", 
        overlap.labels = 1, fontsize.title = 20, fontsize.labels=c(25,18), bg.labels=0, fontcolor.labels=c('white','black'), 
        fontsize.legend=30, force.print.labels=T, ymod.labels=c(0.3,0), palette='RdYlBu')
dev.off()
# Let's see the order the products are placed in baskets and how much of the order they constitute
basket=order_pp[c('order_id', 'department_id', 'add_to_cart_order', 'reordered')]
basket=basket%>%group_by(department_id)

i=0
basket.order=function(data){
  i<<-i+1
  print(i)
  mean=mean(data$add_to_cart_order)
  temp=data[order(data$order_id, data$add_to_cart_order),]
  temp=data%>%group_by(order_id)%>%summarise(first=first(add_to_cart_order))
  temp=mean(temp$first)
  reordered=sum(data$reordered)
  temp=data.frame(first=temp, mean=mean, total=nrow(data), reordered=reordered)
  return(temp)}

basket=group_modify(basket, ~basket.order(.x))
basket$department=departments[basket$department_id,'department']
basket=filter(basket, department!='missing')
basket$reordered=basket$reordered/basket$total

ggplot(data=basket, aes(x=first, y=total, label=str_to_title(department)))+
  geom_point(size=basket$reordered^2*35, color='red', alpha=0.7)+
  labs(title='Department popularity', x='Mean of first add to cart order', y='Total orders')+
  geom_text_repel(box.padding=0.7)

ggplot(data=basket, aes(x=mean, y=total, label=str_to_title(department)))+
  geom_point(size=basket$reordered^2*35, color='blue', alpha=0.7)+
  labs(title='Department popularity', x='Mean of add to cart order', y='Total orders')+
  geom_text_repel(box.padding=0.7)

test=table(order_pp$order_id)
test=data.frame(order_id=names(test), size=as.vector(test))
write.csv(test, 'order_size.csv', row.names=F)

# now we can actually go back to orders and find out the relationship between days_since_prior_order and ordeer size
orders=inner_join(orders, order_size, by='order_id')
summary(order_size$size)
orders.plot=filter(orders, size %between% outlier.range(size, 2), 
                   days_since_prior_order<30, !is.na(days_since_prior_order))
orders.plot=orders.plot%>%group_by(size, days_since_prior_order)%>%summarise(n=n())


png(filename="prior_order_size_tile.png",width=1920, height=1080)
ggplot(data=orders.plot)+
  geom_tile(aes(x=size, y=days_since_prior_order, fill=n))+
  scale_fill_gradient(name='number of orders', low='darkblue', high='orange')+
  scale_x_continuous(breaks=seq(0, max(orders.plot$size), 1), labels=seq(0, max(orders.plot$size), 1))+
  scale_y_continuous(breaks=seq(0,30,1), labels=seq(0,30,1))+
  theme(text = element_text(size=25), legend.key.size = unit(20, 'mm'))+
  labs(title='size of orders vs days since prior order',x='size of order',y='days since prior order')
dev.off()

################################## Feature Engineering and modelling #########################################
# first order of business is to cluster the products so that each aisle is split into many clusters of similar products
# we would do this with the cleaned product names and using ngrams

word.dist=function(word1, word2){
  if(word1=='' | word2==''){return(1)}else{
    word1=strsplit(word1, ' ')[[1]]
    word2=strsplit(word2, ' ')[[1]]
    total.length=length(word1)+length(word2)
    temp=length(c(setdiff(word1, word2),setdiff(word2, word1)))/total.length
    return(temp)
  }}

word.dist.matrix=function(words){
  dim=length(words)
  distance=matrix(nrow=dim, ncol=dim)
  for(i in 1:dim){
    for(k in 1:i){
      val=word.dist(words[i],words[k])
      distance[i,k]=val
      distance[k,i]=val}
    }
  return(distance)
  }

fast.dist=function(data){
  dim.data=nrow(data)
  # x^2 and y^2 and -2xy
  mat=as.matrix(data)
  mat=data%*%t(data)
  diag=diag(mat)
  x2mat=matrix(data=diag, ncol=dim.data, nrow=dim.data)
  y2mat=t(x2mat)
  dist=x2mat+y2mat-2*mat
  dist=sqrt(dist)
  return(round(dist,10))
  }

fast.cos.dist=function(data){
  dim.data=nrow(data)
  mat=as.matrix(data)
  xy=mat%*%t(mat)
  diag=sqrt(diag(xy))
  diag[which(diag==0)]=1
  x2mat=matrix(data=diag, ncol=dim.data, nrow=dim.data)
  y2mat=t(x2mat)
  dist=xy/(x2mat*y2mat)
  return(1-round(dist,10))
  }

count.words=function(data, top=5){
  tokens=Boost_tokenizer(data$product_name)
  temp=table(tokens)
  temp=temp[order(temp, decreasing=T)[1:top]]
  temp=data.frame(word=names(temp), freq=as.vector(temp))
  return(temp)
}


temp=products%>%group_by(aisle_id)%>%summarise(n=n())

test=subset(products, aisle_id==2)
names=VCorpus(VectorSource(test$product_name))
names=DocumentTermMatrix(names)
names=as.matrix(names)
test.dist=as.dist(fast.cos.dist(names))

test.clust=hclust(test.dist, method='ward.D2')
out=outlier.range(test.clust$height)[2]
plot(test.clust)
rect.hclust(test.clust, k=floor(sqrt(nrow(test))))
test=cbind(test, clust=cutree(test.clust, k=floor(sqrt(nrow(test)))))


cluster.tokens=test%>%group_by(clust)%>%group_modify(~count.words(.x, top=3))

i=0
cluster.prod=function(data){
  i<<-i+1
  print(i)
  names=VCorpus(VectorSource(data$product_name))
  names=DocumentTermMatrix(names)
  names=as.matrix(names)
  names.dist=as.dist(fast.cos.dist(names))
  
  prod.clust=hclust(names.dist, method='ward.D2')
  clusters=cutree(prod.clust, k=floor(sqrt(nrow(data))))
  data=cbind(data, clusters)
  return(data)
  }

name.cluster=function(data){
  cluster.size=nrow(data)
  data=Boost_tokenizer(data$product_name)
  data=prop.table(table(data))
  data=data[order(data, decreasing=T)[1:3]]
  data=paste(names(data), collapse=' ')
  return(data.frame(cluster.name=data, cluster.size=cluster.size))
}


test=filter(products.clust, clusters==202)
name.cluster2(test)

products.clust=products%>%group_by(aisle_id)%>%group_modify(~cluster.prod(.x))
products.clust$clusters=products.clust$clusters+100*products.clust$aisle_id
write.csv(products.clust, 'products_clust.csv', row.names=F)

# cluster names now, extracted from the top words of each aisle
cluster.names=products.clust%>%group_by(aisle_id, clusters)%>%group_modify(~name.cluster(.x))
order_pp=inner_join(order_pp, products.clust[c('product_id','clusters')], by='product_id')
cluster.freq=table(order_pp$clusters)
cluster.freq=data.frame(clusters=names(cluster.freq), freq=as.vector(cluster.freq))
cluster.freq$clusters=as.numeric(cluster.freq$clusters)
cluster.freq=inner_join(cluster.freq, cluster.names, by='clusters')
cluster.freq$prop=cluster.freq$freq/sum(cluster.freq$freq)
write.csv(cluster.freq, 'cluster.csv', row.names=F)
#------------------ Association rules --------------------------
# now we have the clustered items, we join it with the orders_pp

transactions=inner_join(order_pp[c('order_id','product_id')], products.clust[c('product_id','clusters')], by='product_id')
transactions=transactions[,-which(names(transactions)=='product_id')]
outlier.products=clusters[which(clusters$prop>quantile(clusters$prop, 0.95)),'clusters']
transactions=filter(transactions, !clusters%in%outlier.products)
transactions=format_csv(transactions)
conn=textConnection(object=transactions)
transactions=read.transactions(conn, 'single', cols=c("order_id","clusters"), header=T, sep = ",")


##transactions=read.transactions('order_transactions.csv', 'single', cols=c("order_id","clusters"), header=T, sep = ",")
supp.min=quantile(clusters$prop, 0.5)
test.arules=apriori(transactions, parameter = list(supp=supp.min, conf = 0.2, target = "rules"))
arules.frame=DATAFRAME(test.arules)


#we've seen that in the cluster frequencies, some items are bought too frequently. It really tainted the associations
# because all the rules with high conidence are with those items. We must remove them with outlier.
string_in=function(x, patterns){
  matched=F
  for(n in patterns){
    if(grepl(n,x)){
      matched=T
      break}
    }
  return(matched)
  }

outlier.products=clusters[which(clusters$prop>supp.max),]
outlier.products=outlier.products[order(outlier.products$prop, decreasing=T),]
outlier.products=as.character(outlier.products[1:100,'clusters'])

arules.frame=arules.frame[which(!sapply(arules.frame$LHS, string_in, patterns=outlier.products)),]
arules.frame=arules.frame[which(!sapply(arules.frame$RHS, string_in, patterns=outlier.products)),]

#------------------------ Looking at reorder ----------
order.reorder=order_pp%>%group_by(order_id)%>%summarise(reorder=sum(reordered), size=max(add_to_cart_order))
order.reorder=mutate(order.reorder, reorder.prop=reorder/size)
order.reorder.plot=order.reorder%>%group_by(size)%>%summarise(mean=mean(reorder.prop), sd=sd())
ggplot(order.reorder.plot)+
  geom_point(aes(x=size, y=mean))+
  geom_line(aes(x=size, y=mean-sd))+
  geom_line(aes(x=size, y=mean+sd))

order_pp=inner_join(order_pp, products[c('product_id','aisle_id')], by='product_id')
test=order_pp
test=test%>%group_by(order_id, aisle_id)%>%summarise(n=n())
test=table(test$order_id, test$aisle_id)
